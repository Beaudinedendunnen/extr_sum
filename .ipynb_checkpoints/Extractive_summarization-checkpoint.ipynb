{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import cluster\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gc\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Beaudine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('dutch') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "# This should be a dataframe consisting of data from a single survey, including the columns for question text \n",
    "# (named: QuestionText) and open answers (named: AnswerText)\n",
    "\n",
    "df = pd.read_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# For illustration: a dataframe consisting of data for a single survey\n",
    "print(list(set(df['SurveyID'])))\n",
    "df = df.loc[df['SurveyID'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe consists of the three columns: SurveyID, AnswerText, QuestionText\n",
    "df = df[['SurveyID', 'AnswerText', 'QuestionText']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------- LOAD MODELS------------------------------------------------------------------\n",
    "# Load embeddings and initialize vocab\n",
    "model = Word2Vec.load('embeddings.bin')\n",
    "vocab = [line.strip().split()[0] for line in model.wv.key_to_index]\n",
    "\n",
    "# load embedding dictionary\n",
    "f = open(\"embedding_dict.pkl\", \"rb\")\n",
    "embedding_dict = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# load TF-IDF model \n",
    "with open('tfidf.mod', 'rb') as f:\n",
    "    tfidf_model = pickle.load(f)\n",
    "    \n",
    "feature_names = tfidf_model.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_for_words(text):\n",
    "    tfidf_matrix= tfidf_model.transform([text]).todense()\n",
    "    feature_index = tfidf_matrix[0,:].nonzero()[1]\n",
    "    tfidf_scores = zip([feature_names[i] for i in feature_index], [tfidf_matrix[0, x] for x in feature_index])\n",
    "    return dict(tfidf_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>=`./?@#$%+^&*_~''') -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    # string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    # string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Removing numbers \n",
    "    string = re.sub('[0-9]+', '', string)\n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    # string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_dist(sentence1, sentence2):\n",
    "\n",
    "    # tokenization\n",
    "    \n",
    "    X= sentence1\n",
    "    Y = sentence2\n",
    "\n",
    "    X_list = word_tokenize(X) \n",
    "\n",
    "    Y_list = word_tokenize(Y)\n",
    "\n",
    "    l1 = []; l2 = []\n",
    "\n",
    "  \n",
    "    # remove stop words from line\n",
    "\n",
    "    X_set = {w for w in X_list if not w in sw} \n",
    "\n",
    "    Y_set = {w for w in Y_list if not w in sw}\n",
    "    \n",
    "    # If any of them is empty, return 0\n",
    "    \n",
    "    if len(X_set) == 0 or len(Y_set) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "\n",
    "    # generate a set containing the keywords of both lines\n",
    "\n",
    " \n",
    "        rvector = X_set.union (Y_set) \n",
    "\n",
    "        vector = X_set.union(Y_set) \n",
    "        for w in rvector:\n",
    "            if w in X_set: l1.append(1) # create a vector\n",
    "            else: l1.append(0)\n",
    "            if w in Y_set: l2.append(1)\n",
    "            else: l2.append(0)\n",
    "        c = 0\n",
    "\n",
    "    # cosine formula \n",
    "        for i in range(len(rvector)):\n",
    "                c+= l1[i]*l2[i]\n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "    \n",
    "        return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_list = [\"en\", \"zo\", \"zij\", \"hij\", \"zowel\", \"zoals\", \"waarmee\", \"zodat\", \"ook\", \"vervolgens\", \"maar\", \"echter\", \"toch\", \"daarentegen\", \"want\", \"omdat\", \\\n",
    "                  \"daarnaast\", \"verder\", \"daarom\", \"ook\", \"mits\", \"tenzij\", \"waardoor\", \"daardoor\", \"zodoende\", \"dus\", \"vandaar\", \"daarmee\"]\n",
    "\n",
    "false_ending_list = [\"bijv\", \"bv\", \"etc\", \"mi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create summary algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(selection, summary_df):    \n",
    "    \n",
    "    summaries_selected = df.loc[df['QuestionText'] == selection]\n",
    "    \n",
    "    df_summary = summaries_selected['AnswerText']\n",
    "            \n",
    "    questions = summaries_selected['QuestionText']\n",
    "\n",
    "    question = ''\n",
    "    for count, i in enumerate(questions):\n",
    "        if count == 0:\n",
    "            question += i\n",
    "        \n",
    "    # Clean \n",
    "    texts = []\n",
    "    for i in tqdm(df_summary):\n",
    "        texts.append(str(i))\n",
    "        \n",
    "    sentences = []\n",
    "    for i in texts:\n",
    "        i = sent_tokenize(i)\n",
    "        for j in i:\n",
    "            sentences.append(j)\n",
    "    del texts \n",
    "    gc.collect()\n",
    "    sentences_cleaned = []\n",
    "    for sent in tqdm(sentences): \n",
    "        sent = clean_text(sent)\n",
    "        sentences_cleaned.append(sent)\n",
    "        \n",
    "    small_sentences = []\n",
    "    for sentence in tqdm(sentences_cleaned):\n",
    "        sentence_dict = get_tfidf_for_words(sentence)\n",
    "        new_sentence = \"\"\n",
    "        for word in sentence.split():\n",
    "            if word in feature_names and sentence_dict[word] >0.3:\n",
    "                new_sentence = new_sentence + word\n",
    "                new_sentence = new_sentence + \" \"\n",
    "        small_sentences.append(new_sentence)\n",
    "        \n",
    "    new_old_dict = dict(zip(small_sentences, sentences_cleaned))\n",
    "    \n",
    "    sentence_vectors = []\n",
    "    for i in tqdm(small_sentences):\n",
    "        if len(i) > 0:\n",
    "            v = sum([embedding_dict.get(w, np.zeros((100,))) for w in i.split() if w in vocab])/((len(i.split())+0.001))\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "        \n",
    "    sentence_vectors_cleaned = []\n",
    "    for i in tqdm(sentence_vectors):\n",
    "        if type(i) == float:\n",
    "            i = np.zeros((100,))\n",
    "        sentence_vectors_cleaned.append(i)\n",
    "        \n",
    "    sentence_vectors = sentence_vectors_cleaned\n",
    "    del sentence_vectors_cleaned \n",
    "    gc.collect()\n",
    "    \n",
    "    sentence_vectors_dictionary = {}\n",
    "    for i in tqdm(small_sentences):\n",
    "        if len(i) != 0:\n",
    "            sentence_vectors_dictionary[i] = sum([embedding_dict.get(w, np.zeros((100,))) for w in i.split() if w in vocab])/((len(i.split())+0.001))\n",
    "        else: \n",
    "            sentence_vectors_dictionary[i] = np.zeros((100,))\n",
    "        \n",
    "    pca = PCA(2)\n",
    "    sentence_vectors_pca = pca.fit_transform(sentence_vectors)\n",
    "    k = 0\n",
    "    if len(sentence_vectors_pca) > 500:\n",
    "        k = 10 \n",
    "    elif len(sentence_vectors_pca) > 100 and len(sentence_vectors_pca) <= 500:\n",
    "        k = 8 \n",
    "    else: \n",
    "        k = 6 \n",
    "        \n",
    "    kmeans_pca = KMeans(n_clusters=k, random_state=0)\n",
    "    label_pca = kmeans_pca.fit_predict(sentence_vectors_pca)\n",
    "    centroids_pca = kmeans_pca.cluster_centers_\n",
    "    \n",
    "    labels_df = pd.DataFrame(label_pca)\n",
    "    vectors = []\n",
    "    for i in sentence_vectors_pca:\n",
    "        vectors.append(i)\n",
    "    labels_df['vectors'] = vectors\n",
    "\n",
    "    datapoint = []\n",
    "    for vector in tqdm(sentence_vectors):\n",
    "        count = 0\n",
    "        for key, item in sentence_vectors_dictionary.items():\n",
    "            if np.array_equiv(vector, item) == True and count == 0:\n",
    "                datapoint.append(key)\n",
    "                count += 1\n",
    "\n",
    "    new_datapoints = []\n",
    "    for i in datapoint:\n",
    "        new_datapoint = new_old_dict[i]\n",
    "        new_datapoints.append(new_datapoint)\n",
    "    \n",
    "    labels_df['sentence'] = new_datapoints\n",
    "\n",
    "    labels_df.rename(columns = {0:'label'}, inplace = True)\n",
    "    \n",
    "    tfidf_scores = []\n",
    "    for sentence in labels_df['sentence']:\n",
    "        summed = 0\n",
    "        for key, value in get_tfidf_for_words(sentence).items():\n",
    "            summed += value\n",
    "        tfidf_scores.append(summed)\n",
    "        \n",
    "    labels_df['tfidf'] = tfidf_scores\n",
    "    \n",
    "    length_list = []\n",
    "    for sentence in tqdm(labels_df['sentence']):\n",
    "        length = 0\n",
    "        for word in sentence.split():\n",
    "            length += 1 \n",
    "        length_list.append(length)\n",
    "    \n",
    "    labels_df['length'] = length_list\n",
    "\n",
    "    cosine_list = []\n",
    "    for index, sentence1 in tqdm(enumerate(labels_df[\"sentence\"])):\n",
    "        cosine_sum = 0\n",
    "        label = labels_df['label'][index]\n",
    "        for sentence2 in labels_df.loc[labels_df['label'] == label]['sentence']:\n",
    "            cosine_sum += cosine_dist(sentence1, sentence2)\n",
    "        cosine_list.append(cosine_sum)\n",
    "        \n",
    "    labels_df['cosine'] = cosine_list\n",
    "    \n",
    "    normalize_labels_df = labels_df[['tfidf', 'length', 'cosine']]\n",
    "\n",
    "    normalized_pca=(normalize_labels_df-normalize_labels_df.min())/(normalize_labels_df.max()-normalize_labels_df.min())\n",
    "    \n",
    "    normalized_pca['sum'] = normalized_pca.sum(axis=1)\n",
    "    \n",
    "    labels_df['score'] = normalized_pca['sum']\n",
    "    labels_df = labels_df[['label', 'sentence', 'score']]\n",
    "    \n",
    "    \n",
    "    df_pca_grouped =  labels_df.groupby(\"label\")\n",
    "    max_df_pca = df_pca_grouped.max()\n",
    "    max_df_pca = max_df_pca.reset_index()\n",
    "\n",
    "\n",
    "    label_count=collections.Counter(label_pca)\n",
    "\n",
    "# Create some dictionaries that can help convert quickly (for example when we have the label and want the sentence,\n",
    "# or when we have the sentence but need the cluster count)\n",
    "        \n",
    "    label_sent_dict = {}    \n",
    "    for label, sentence in enumerate(max_df_pca['sentence']):\n",
    "        label_sent_dict[label] = sentence\n",
    "\n",
    "    sent_count_dict = {}\n",
    "    for label, sentence in label_sent_dict.items():\n",
    "        count = label_count[label]\n",
    "        sent_count_dict[sentence] = count\n",
    "\n",
    "    count_sent_dict = {}\n",
    "    for label, sentence in label_sent_dict.items():\n",
    "        count = label_count[label]\n",
    "        count_sent_dict[count] = sentence\n",
    "\n",
    "    count_list = []\n",
    "    for value in sent_count_dict.values():\n",
    "        count_list.append(value)\n",
    "    \n",
    "    total_count = sum(count_list)\n",
    "\n",
    "    count_list.sort(reverse=True)\n",
    "\n",
    "# Create a sorted list of selected sentences for summary generation \n",
    "\n",
    "    ordered_dict = collections.OrderedDict(sorted(count_sent_dict.items(), reverse=True))\n",
    "\n",
    "    sorted_sentences = []\n",
    "    for count, sent in ordered_dict.items():\n",
    "        sorted_sentences.append(sent)\n",
    "\n",
    "    \n",
    "# In case you want to only use large clusters in the summary, use this list instead of the sorted sentences list. \n",
    "# This list is now set at a frequency higher than 5%, but this can be adjusted where necessary \n",
    "    sorted_sentences_high_frequency = []\n",
    "    for sentence in sorted_sentences:\n",
    "        if sent_count_dict[sentence]*100/total_count >= 1:\n",
    "            sorted_sentences_high_frequency.append(sentence)\n",
    "\n",
    "    \n",
    "    summary = []\n",
    "\n",
    "    for iteration, summary_sentence in enumerate(sorted_sentences):\n",
    "        count = 0\n",
    "        for index, original_sentence in enumerate(sentences):\n",
    "            if summary_sentence == clean_text(original_sentence) and count == 0:\n",
    "                percentage = round((sent_count_dict[summary_sentence]*100)/total_count, 1)\n",
    "                if iteration == 0:\n",
    "                    intro = \"To the question: \\\"\" + question + \"\\\" the following grouped answers can be found.\"\n",
    "                    to_add = \"The largest amount of respondents, \" + str(percentage) + \"%, say the following:\"\n",
    "                    summary.append(intro)\n",
    "                    #summary.append(\"\\n\")\n",
    "                    summary.append(to_add)\n",
    "                elif percentage >= 5:\n",
    "                    to_add = str(percentage) + \"% of the answers say the following:\"\n",
    "                    summary.append(to_add)\n",
    "                else:\n",
    "                    summary.append(\"Less than 5%) of the answers concern the following: \")\n",
    "                words = clean_text(original_sentence).split()\n",
    "                if words[0] in connector_list:\n",
    "                    summary.append(sentences[index-1])\n",
    "                    summary.append(original_sentence)\n",
    "                    #summary.append(\"\\n\")\n",
    "                    count += 1\n",
    "                elif words[-1] in false_ending_list:\n",
    "                    summary.append(original_sentence + \" \" + sentences[index+1])\n",
    "                    #summary.append(\"\\n\")\n",
    "                    count += 1\n",
    "                else:\n",
    "                    summary.append(original_sentence)\n",
    "                    #summary.append(\"\\n\")\n",
    "                    count += 1\n",
    "    \n",
    "    final_summary = ''\n",
    "    for i in summary:\n",
    "        final_summary += i\n",
    "        final_summary += \"\\n\"\n",
    "        \n",
    "            \n",
    "    return final_summary\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_summary('text', df))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
